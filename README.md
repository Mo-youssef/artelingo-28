# No Culture Left Behind: ArtELingo-28, a Benchmark of WikiArt with Captions in 28 Languages
[Youssef Mohamed](https://mo-youssef.github.io/), [Runjia Li](https://runjiali-rl.github.io/), [Ibrahim Said Ahmad](https://isahmadbbr.github.io/), [Kilichbek Haydarov](https://kilichbek.github.io/webpage/), [Philip Torr](https://eng.ox.ac.uk/people/philip-torr/), [Kenneth Ward Church](https://www.khoury.northeastern.edu/people/kenneth-church/) and [Mohamed Elhoseiny](https://www.mohamed-elhoseiny.com/).


## News
Our paper was accepted at EMNLP'24 


## Abstract
Research in vision and language has made considerable progress thanks to benchmarks such as COCO. COCO captions focused on unambiguous facts in English; ArtEmis introduced subjective emotions and ArtELingo introduced some multilinguality (Chinese and Arabic). However we believe there should be more multilinguality. Hence, we present ArtELingo-28, a vision-language benchmark that spans 28 languages and encompasses approximately 200,000 annotations (140 annotations per image). Traditionally, vision research focused on unambiguous class labels, whereas ArtELingo-28 emphasizes diversity of opinions over languages and cultures. The challenge is to build machine learning systems that assign emotional captions to images. Baseline results will be presented for three novel conditions: Zero-Shot, Few-Shot and One-vs-All Zero-Shot. We find that cross-lingual transfer is more successful for culturally-related languages. 


## Getting Started
This repo is a branch from [MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4)

For installation instructions please follow the same instructions in MiniGPT-4


## License
This repository is under [BSD 3-Clause License](LICENSE.md).
Many codes are based on [Lavis](https://github.com/salesforce/LAVIS) with 
BSD 3-Clause License [here](LICENSE_Lavis.md).
